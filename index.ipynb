{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that you have a working knowledge of CNNs and have practiced implementing associated techniques in Keras, its time to put all of those skills together. In this lab, you'll work to complete a Kaggle competition on classifying dog breeds.\n",
    "\n",
    "https://www.kaggle.com/c/dog-breed-identification\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Independently design and build a CNN for image classifcation tasks\n",
    "* Compare and apply multiple techniques for tuning a model including data augmentation and adapting pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Load the Data\n",
    "\n",
    "Start by downloading the data locally and loading it into a Pandas DataFrame. Be forewarened that this dataset is fairly large and it is advisable to close other memory intensive applications.\n",
    "\n",
    "The data can be found here:\n",
    "\n",
    "https://www.kaggle.com/c/dog-breed-identification/data\n",
    "\n",
    "We recommend downloading the data into this directory on your local computer. From there, be sure to uncompress the folder and subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No code persay, but download and decompress the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Now that you've downloaded the data, its time to prepare it for some model building! You'll notice that the current structure provided is not the same as our lovely preprocessed folders that we've been providing you. Instead, you have one large training folder with images and a csv file with labels associated with each of these file types. \n",
    "\n",
    "Use this to create a directory substructure for a train-validation-test split as we have done previously. Also recall from our previous work that you'll also want to use one-hot encoding as we are now presented with a multi-class problem as opposed to simple binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; open the labels.csv file stored in the zip file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to create our standard directory structure:\n",
    "* train\n",
    "    * category1\n",
    "    * category2\n",
    "    * category3\n",
    "    ...\n",
    "* val\n",
    "    * category1\n",
    "    * category2\n",
    "    * category3\n",
    "    ...\n",
    "* test \n",
    "    * category1\n",
    "    * category2\n",
    "    * category3\n",
    "    ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, shutil\n",
    "\n",
    "old_dir = 'train/'\n",
    "\n",
    "new_root_dir = 'data_org/'\n",
    "os.mkdir(new_root_dir)\n",
    "\n",
    "dir_names = ['train', 'val', 'test']\n",
    "for d in dir_names:\n",
    "    new_dir = os.path.join(new_root_dir, d)\n",
    "    os.mkdir(new_dir)\n",
    "    \n",
    "for breed in df.breed.unique():\n",
    "    print('Moving {} pictures.'.format(breed))\n",
    "    #Create sub_directories\n",
    "    for d in dir_names:\n",
    "        new_dir = os.path.join(new_root_dir, d, breed)\n",
    "        os.mkdir(new_dir)\n",
    "    #Subset dataframe into train, validate and split sets\n",
    "    #Split is performed here to ensure maintain class distributions.\n",
    "    temp = df[df.breed == breed]\n",
    "    train, validate, test = np.split(temp.sample(frac=1), [int(.8*len(temp)), int(.9*len(temp))])\n",
    "    print('Split {} imgs into {} train, {} val, and {} test examples.'.format(len(temp),\n",
    "                                                                              len(train),\n",
    "                                                                              len(validate),\n",
    "                                                                              len(test)))\n",
    "    for i, temp in enumerate([train, validate, test]):\n",
    "        for row in temp.index:\n",
    "            filename = temp['id'][row] + '.jpg'\n",
    "            origin = os.path.join(old_dir + filename)\n",
    "            destination = os.path.join(new_root_dir + dir_names[i] + '/' + breed + '/' + filename)\n",
    "            shutil.copy(origin, destination)\n",
    "#Your code here\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = 'data_org/train'\n",
    "validation_dir = 'data_org/val/'\n",
    "test_dir = 'data_org/test/'\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Build a Baseline CNN\n",
    "\n",
    "This is an optional step. Adapting a pretrained model will produce better results, but it may be interesting to create a CNN from scratch as a baseline. If you wish to, do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a baseline CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with the Pretrained Model\n",
    "\n",
    "Now that you've loaded a pretrained model, it's time to adapt that convolutional base and add some fully connected layers on top in order to build a classifier from these feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; add fully connected layers on top of the convolutional base\n",
    "\n",
    "from keras.applications import VGG19\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "cnn_base = VGG19(weights = 'imagenet',\n",
    "                include_top = False,\n",
    "                input_shape = (240,240,3))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(cnn_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(128, activation = 'relu'))\n",
    "model.add(layers.Dense(256, activation = 'relu'))\n",
    "model.add(layers.Dense(128, activation = 'relu'))\n",
    "model.add(layers.Dense(120, activation = 'sigmoid'))\n",
    "\n",
    "cnn_base.trainable = False\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer.trainable)\n",
    "    \n",
    "print(model.trainable_weights)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "train_dir = 'data_org/train'\n",
    "val_dir = 'data_org/val'\n",
    "test_dir = 'data_org/test'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  rotation_range = 40,\n",
    "                                  width_shift_range = .2,\n",
    "                                  height_shift_range = .2,\n",
    "                                  shear_range = .2,\n",
    "                                  zoom_range = .2,\n",
    "                                  horizontal_flip = True,\n",
    "                                  fill_mode = 'nearest')\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    target_size = (240,240),\n",
    "                                                    batch_size = 20,\n",
    "                                                    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = ImageDataGenerator(rescale = 1./255).flow_from_directory(val_dir,\n",
    "                                                    target_size = (240,240),\n",
    "                                                    batch_size = 20,\n",
    "                                                    class_mode = 'categorical')\n",
    "\n",
    "\n",
    "test_generator = ImageDataGenerator(rescale = 1./255).flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (240,240),\n",
    "    batch_size = 180,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False\n",
    "                                                                         \n",
    "                                                                         )\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = optimizers.RMSprop(lr = 2e-5),\n",
    "              metrics = ['acc']\n",
    "             \n",
    "             )\n",
    "\n",
    "history = model.fit_geneator(train_generator,\n",
    "                             steps_per_epoch = 25,\n",
    "                             epochs = 12,\n",
    "                             validation_data = val_generator,\n",
    "                             validation_steps = 10\n",
    ")\n",
    "                       \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize History\n",
    "\n",
    "Now fit the model and visualize the training and validation accuracy/loss functions over successive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; visualize the training / validation history associated with fitting the model.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "model.save('vgg19_FE_AUG_10epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "test_generator = test_datagen.flow_from_dictionary(\n",
    "    test_dir,\n",
    "    target_size = (240,240),\n",
    "    batch_size = 20,\n",
    "    class_mode = 'categorical',\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps = 54)\n",
    "predictions = model.predict_generator(test_generator, steps = 54)\n",
    "print('number of predictions: {}'.format(len(predictions)))\n",
    "print('accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! In this lab, you brought all of your prior deep learning skills together from preprocessing including one-hot encoding, to adapting a pretrained model. There are always ongoing advancements in CNN architectures and best practices, but you have a solid foundation and understanding at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
